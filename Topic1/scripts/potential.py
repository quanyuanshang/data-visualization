"""
Export Potential Artists with SHAP Explanations for Visualization.
(Fixed: Ensure leverage_ratio is included in SHAP calculation)
"""
import argparse
import pickle
import json
import pandas as pd
import shap
import numpy as np
from pathlib import Path

# ================= é…ç½®åŒºåŸŸ =================
# è¾“å…¥æ–‡ä»¶è·¯å¾„
INPUT_PREDS = Path("output/artist_success_predictions.csv")
INPUT_FEATURES = Path("output/artist_features.parquet")
INPUT_MODEL = Path("output/artist_success_xgb.pkl")

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(r"D:\cs\æ•°æ®å¯è§†åŒ–\Topic1\genre-visualization\public\data")

# ç­›é€‰é˜ˆå€¼
FILTER_CONFIG = {
    "max_target_score": 1.3,      
    "max_total_works": 10,         
    "min_neighbor_pr": 1e-9,      
    "min_predicted_score": 2   
}
# ===========================================

def main():
    print(f"ğŸš€ Starting export process...")
    
    if not (INPUT_PREDS.exists() and INPUT_FEATURES.exists() and INPUT_MODEL.exists()):
        print(f"âŒ Error: Input files not found in 'output/' directory.")
        return

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # 1. åŠ è½½æ•°æ®
    print("ğŸ“¥ Loading data and model...")
    df_preds = pd.read_csv(INPUT_PREDS)
    df_features = pd.read_parquet(INPUT_FEATURES)
    
    with open(INPUT_MODEL, "rb") as f:
        model = pickle.load(f)

    # 2. åˆå¹¶æ•°æ®
    # ç¡®ä¿ leverage_ratio å­˜åœ¨ã€‚å¦‚æœç‰¹å¾æ–‡ä»¶é‡Œæ²¡æœ‰ï¼ˆå¯èƒ½åœ¨è®­ç»ƒè„šæœ¬é‡Œè®¡ç®—äº†ä½†æ²¡å­˜è¿›parquetï¼‰ï¼Œè¿™é‡Œéœ€è¦é‡æ–°è®¡ç®—å¹¶è¡¥å…¨
    # æ£€æŸ¥ df_features æ˜¯å¦åŒ…å« leverage_ratioï¼Œå¦‚æœæ²¡æœ‰ï¼Œæ‰‹åŠ¨è¡¥ä¸Š
    if "leverage_ratio" not in df_features.columns and "max_neighbor_pr" in df_features.columns and "pagerank" in df_features.columns:
        print("âš ï¸ 'leverage_ratio' missing in features, recalculating...")
        df_features["leverage_ratio"] = df_features["max_neighbor_pr"] / (df_features["pagerank"] + 1e-9)

    df_full = pd.merge(
        df_preds[["person_id", "name", "target_score", "predicted_score", "residual"]],
        df_features, # ç›´æ¥åˆå¹¶æ‰€æœ‰ç‰¹å¾åˆ—
        on="person_id",
        how="inner",
        suffixes=("", "_dup") # é˜²æ­¢é‡å¤åˆ—æŠ¥é”™
    )
    # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„é‡å¤åˆ—
    df_full = df_full.loc[:, ~df_full.columns.str.endswith('_dup')]

    # 3. ç­›é€‰é€»è¾‘
    print("ğŸ” Filtering for 'Co-Signed' potential artists...")
    
    # ç¡®ä¿ç”¨äºç­›é€‰çš„åˆ—å­˜åœ¨
    if "leverage_ratio" not in df_full.columns:
         df_full["leverage_ratio"] = df_full["max_neighbor_pr"] / (df_full["pagerank"] + 1e-9)

    mask = (
        (df_full["target_score"] < FILTER_CONFIG["max_target_score"]) &
        (df_full["total_works"] <= FILTER_CONFIG["max_total_works"]) &
        (df_full["max_neighbor_pr"] > FILTER_CONFIG["min_neighbor_pr"]) &
        (df_full["predicted_score"] > FILTER_CONFIG["min_predicted_score"])
    )
    
    candidates = df_full[mask].copy()
    candidates = candidates.sort_values(by="leverage_ratio", ascending=False)
    
    print(f"ğŸ’ Found {len(candidates)} candidates matching criteria.")
    
    if candidates.empty:
        print("âš ï¸ No candidates found! Falling back to Top 10 by leverage...")
        mask_fallback = (
            (df_full["target_score"] < FILTER_CONFIG["max_target_score"]) & 
            (df_full["max_neighbor_pr"] > 0)
        )
        candidates = df_full[mask_fallback].sort_values(by="leverage_ratio", ascending=False).head(10)

    # 4. å¯¼å‡º CSV
    csv_output_path = OUTPUT_DIR / "potential_artists_list.csv"
    cols_to_save = [
        "person_id", "name", "total_works", "target_score", 
        "predicted_score", "max_neighbor_pr", "leverage_ratio", "pagerank"
    ]
    candidates[cols_to_save].to_csv(csv_output_path, index=False)
    print(f"âœ… Saved CSV list to: {csv_output_path}")

    # 5. SHAP åˆ†æ (å…³é”®ä¿®å¤éƒ¨åˆ†)
    print("ğŸ§  Calculating SHAP values for visualization...")
    
    # === ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ ===
    if hasattr(model, "feature_names_in_"):
        # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹è®°å½•çš„ç‰¹å¾åï¼ˆé¡ºåºå’Œåç§°å¿…é¡»å®Œå…¨åŒ¹é…ï¼‰
        feature_cols = list(model.feature_names_in_)
        print(f"   Using {len(feature_cols)} features defined in model.")
    else:
        # å¤‡é€‰æ–¹æ¡ˆï¼šæ’é™¤éç‰¹å¾åˆ—
        # ã€é‡è¦ã€‘ è¿™é‡Œåˆ æ‰äº† 'leverage_ratio'ï¼Œå› ä¸ºå®ƒç°åœ¨æ˜¯ç‰¹å¾ä¹‹ä¸€äº†ï¼
        exclude = {"person_id", "name", "target_score", "predicted_score", "residual"} 
        feature_cols = [c for c in df_features.columns if c not in exclude]
        print(f"   Using {len(feature_cols)} features (inferred).")

    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å¯¹é½
    missing_cols = [c for c in feature_cols if c not in candidates.columns]
    if missing_cols:
        print(f"âŒ Error: The following features are missing in the data: {missing_cols}")
        print("   Please make sure 'artist_features.parquet' contains all training features.")
        return

    explainer = shap.TreeExplainer(model)
    viz_data = []
    
    for idx, row in candidates.iterrows():
        # æå–å•è¡Œç‰¹å¾çŸ©é˜µ (ä¸¥æ ¼æŒ‰ç…§ feature_cols é¡ºåº)
        X_single = pd.DataFrame([row[feature_cols]])
        
        # è®¡ç®— SHAP
        shap_obj = explainer(X_single)
        base_value = float(shap_obj.base_values[0])
        shap_values = shap_obj.values[0]
        
        # æ•´ç† JSON
        contributions = []
        for feat_name, feat_val, impact in zip(feature_cols, X_single.iloc[0], shap_values):
            if abs(impact) > 0.001:
                contributions.append({
                    "feature": feat_name,
                    "value": float(feat_val),
                    "impact": float(impact)
                })
        
        contributions.sort(key=lambda x: abs(x["impact"]), reverse=True)
        
        viz_data.append({
            "id": int(row["person_id"]),
            "name": row["name"],
            "metrics": {
                "total_works": int(row["total_works"]),
                "target_score": float(row["target_score"]),
                "predicted_score": float(row["predicted_score"]),
                "leverage_ratio": float(row.get("leverage_ratio", 0)),
                "max_neighbor_pr": float(row.get("max_neighbor_pr", 0))
            },
            "shap_explanation": {
                "base_value": base_value,
                "final_value": float(base_value + sum(shap_values)),
                "factors": contributions
            }
        })

    json_output_path = OUTPUT_DIR / "potential_artists_shap_viz.json"
    with open(json_output_path, "w", encoding="utf-8") as f:
        json.dump(viz_data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… Saved JSON for visualization to: {json_output_path}")

if __name__ == "__main__":
    main()